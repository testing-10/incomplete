# Foundation Model Testing Framework

A comprehensive framework for testing and evaluating foundation language models from OpenAI and Anthropic.

## Overview

This testing framework allows you to evaluate and compare the performance of various foundation models across multiple dimensions, including:

- Factual accuracy
- Reasoning capabilities
- Instruction following
- Extended context handling
- Performance metrics (latency, cost)

The framework is designed to be extensible, allowing you to add new models, test cases, and evaluation metrics as needed.

## Features

- Support for OpenAI models (GPT-4o, GPT-4 Turbo, GPT-4.1, GPT-4.5)
- Support for Anthropic models (Claude 3 Opus, Sonnet, Haiku, Claude 3.7 Sonnet)
- Configurable test suites for different capabilities
- Detailed metrics and reporting
- Parallel testing for efficiency
- Comprehensive result storage and analysis

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/foundation-model-testing.git
   cd foundation-model-testing